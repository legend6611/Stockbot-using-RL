<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning for Stock Trading</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f7fa;
        }
        header {
            text-align: center;
            padding: 30px 0;
            background-color: #1a365d;
            color: white;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        h1 {
            margin: 0;
            font-size: 2.5rem;
        }
        .subtitle {
            font-size: 1.2rem;
            opacity: 0.8;
        }
        .card-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            gap: 20px;
            margin-bottom: 40px;
        }
        .card {
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            overflow: hidden;
            width: calc(33% - 20px);
            min-width: 300px;
            transition: transform 0.3s ease;
        }
        .card:hover {
            transform: translateY(-5px);
        }
        .card-header {
            background-color: #2c5282;
            color: white;
            padding: 15px 20px;
        }
        .card-header h2 {
            margin: 0;
            font-size: 1.5rem;
        }
        .card-body {
            padding: 20px;
        }
        .metrics {
            display: flex;
            align-items: center;
            justify-content: space-between;
            background-color: #ebf4ff;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .metrics-label {
            font-weight: bold;
            color: #2a4365;
        }
        .metrics-value {
            font-size: 1.8rem;
            font-weight: bold;
            color: #2b6cb0;
        }
        .model-image {
            width: 100%;
            border-radius: 5px;
            margin-bottom: 20px;
            overflow: hidden;
        }
        .model-image img {
            width: 100%;
            display: block;
        }
        .download-btn {
            display: block;
            background-color: #3182ce;
            color: white;
            text-align: center;
            padding: 12px;
            text-decoration: none;
            border-radius: 5px;
            font-weight: bold;
            transition: background-color 0.3s ease;
        }
        .download-btn:hover {
            background-color: #2c5282;
        }
        .project-info {
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            margin-bottom: 30px;
        }
        .reward-function {
            background-color: #e6f7ff;
            border-left: 4px solid #1890ff;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        code {
            font-family: 'Consolas', 'Monaco', monospace;
            background-color: #f0f2f5;
            padding: 2px 5px;
            border-radius: 3px;
            font-size: 0.9rem;
        }
        footer {
            text-align: center;
            padding: 20px;
            color: #718096;
            font-size: 0.9rem;
        }
        @media (max-width: 768px) {
            .card-container {
                flex-direction: column;
            }
            .card {
                width: 100%;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Reinforcement Learning for Stock Trading</h1>
        <p class="subtitle">Analysis of Reliance Stock (2020-2025)</p>
    </header>

    <section class="project-info">
        <h2>Project Overview</h2>
        <p>This project explores the application of reinforcement learning algorithms to stock trading, using Reliance Industries stock data from 2020 to March 2025. We trained and compared three popular RL algorithms: Trust Region Policy Optimization (TRPO), Deep Q-Network (DQN), and Proximal Policy Optimization (PPO).</p>
        <p>Our results demonstrate that reinforcement learning can be effectively applied to stock trading strategies, with TRPO achieving the highest profit multiplier of 1.56, followed by PPO at 1.38, and DQN at 1.08.</p>
    </section>

    <section class="project-info">
        <h2>Reward Function</h2>
        <p>The reward function is a critical component of our reinforcement learning models. It determines how the agent evaluates the success of its actions and guides the learning process.</p>
        
        <div class="reward-function">
            <h3>Our Trading Reward Function:</h3>
            <p>For each trading period <code>t</code>, the reward <code>R(t)</code> is calculated as:</p>
            <code>R(t) = (position_value(t) - position_value(t-1)) - transaction_cost(t)</code>
            <p>Where:</p>
            <ul>
                <li><code>position_value(t)</code> is the total value of the portfolio at time <code>t</code></li>
                <li><code>transaction_cost(t)</code> represents trading fees and slippage</li>
            </ul>
            <p>Additionally, we incorporate risk-adjustment factors:</p>
            <ul>
                <li>Drawdown penalty to discourage excessive risk-taking</li>
                <li>Volatility penalty to promote stable returns</li>
                <li>Holding period bonus to encourage strategic long-term positions</li>
            </ul>
            <p>The total reward shown in our results represents the cumulative sum of all rewards throughout the testing period, while the profit multiplier represents the final portfolio value divided by the initial investment.</p>
        </div>
    </section>

    <section>
        <h2>Model Performance</h2>
        <div class="card-container">
            <!-- TRPO Model Card -->
            <div class="card">
                <div class="card-header">
                    <h2>TRPO Model</h2>
                </div>
                <div class="card-body">
                    <div class="metrics">
                        <span class="metrics-label">Profit Multiplier</span>
                        <span class="metrics-value">1.56x</span>
                    </div>
                    <div class="metrics">
                        <span class="metrics-label">Total Reward</span>
                        <span class="metrics-value">509.07</span>
                    </div>
                    <div class="model-image">
                        <img src="Screenshot 2025-04-01 220557.png" alt="TRPO Performance Chart showing profit growth to 1.56x" />
                    </div>
                    <p><strong>Trust Region Policy Optimization (TRPO)</strong> is an advanced policy gradient method that guarantees monotonic improvement through constrained optimization. It performed the best in our tests, achieving a 56% return on investment.</p>
                    <p>TRPO excels at maintaining stable policy updates, making it especially suitable for volatile stock markets where abrupt strategy changes can be detrimental.</p>
                    <a href="trpo.ipynb" class="download-btn" download>Download TRPO Model</a>
                </div>
            </div>

            <!-- PPO Model Card -->
            <div class="card">
                <div class="card-header">
                    <h2>PPO Model</h2>
                </div>
                <div class="card-body">
                    <div class="metrics">
                        <span class="metrics-label">Profit Multiplier</span>
                        <span class="metrics-value">1.36x</span>
                    </div>
                    <div class="metrics">
                        <span class="metrics-label">Total Reward</span>
                        <span class="metrics-value">350.03</span>
                    </div>
                    <div class="model-image">
                        <img src="Screenshot 2025-04-01 224642.png" alt="PPO Performance Chart showing profit growth to 1.36x" />
                    </div>
                    <p><strong>Proximal Policy Optimization (PPO)</strong> simplifies TRPO while maintaining its benefits. It achieved a 36% return, making it our second-best performer with a good balance of performance and computational efficiency.</p>
                    <p>PPO uses a clipped objective function to prevent too large policy updates, providing more stable training than simpler methods like DQN.</p>
                    <a href="rl_stock_model_(1).ipynb" class="download-btn" download>Download PPO Model</a>
                </div>
            </div>

            <!-- DQN Model Card -->
            <div class="card">
                <div class="card-header">
                    <h2>DQN Model</h2>
                </div>
                <div class="card-body">
                    <div class="metrics">
                        <span class="metrics-label">Profit Multiplier</span>
                        <span class="metrics-value">1.08x</span>
                    </div>
                    <div class="metrics">
                        <span class="metrics-label">Total Reward</span>
                        <span class="metrics-value">296.76</span>
                    </div>
                    <div class="model-image">
                        <img src="Screenshot 2025-04-01 220410.png" alt="DQN Performance Chart showing profit growth to 1.08x" />
                    </div>
                    <p><strong>Deep Q-Network (DQN)</strong> combines Q-learning with deep neural networks. While it showed the lowest performance with only an 8% return, it remains valuable for its simplicity and ease of implementation.</p>
                    <p>DQN uses experience replay and target networks to stabilize training, but our tests show it may struggle with the continuous action space of stock trading compared to policy gradient methods.</p>
                    <a href="Untitled6.ipynb" class="download-btn" download>Download DQN Model</a>
                </div>
            </div>
        </div>
    </section>

    <section class="project-info">
        <h2>Methodology</h2>
        <p>We used the following approach for our reinforcement learning stock trading project:</p>
        <ul>
            <li><strong>Data:</strong> Daily price data for Reliance Industries from January 2020 to March 2025</li>
            <li><strong>State Space:</strong> Technical indicators including price, moving averages, RSI, MACD, and volume</li>
            <li><strong>Action Space:</strong> Buy, sell, and hold positions with varying position sizes</li>
            <li><strong>Training:</strong> 80% of data used for training, 20% for testing</li>
        </ul>
        <p>All models were implemented using Python with TensorFlow and the Stable Baselines3 library. Hyperparameters were optimized using a combination of grid search and Bayesian optimization.</p>
    </section>

    <section class="project-info">
        <h2>Conclusion</h2>
        <p>Our research demonstrates the effectiveness of reinforcement learning algorithms in stock trading, particularly with Reliance stock. The TRPO model achieved the best performance with a 56% profit, followed by PPO with 36% and DQN with 8%.</p>
        <p>The results suggest that policy gradient methods (TRPO and PPO) are particularly well-suited for stock trading tasks, likely due to their ability to handle continuous action spaces and their more stable policy updates compared to value-based methods like DQN.</p>
        <p>Future work could explore combining these models in an ensemble approach or incorporating additional market data sources to further improve performance.</p>
    </section>

    <footer>
        <p>2025 Reinforcement Learning Stock Trading Project</p>
    </footer>
</body>
</html>